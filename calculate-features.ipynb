{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime as dt\n",
    "from natsort import natsorted\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams.update({'legend.fontsize': 'large',\n",
    "                     'axes.labelsize': 'large',\n",
    "                     'axes.titlesize': 'large',\n",
    "                     'xtick.labelsize': 'large',\n",
    "                     'ytick.labelsize': 'large'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about extracting and calculating features needed in my thesis. \n",
    "\n",
    "Originally data is provided in matlab form, each day of market data in separate files. At first I will read the data using Python pandas, extract bid size&price and ask price&size with timestamp into csv file ($v_1$ in the picture below). This is done file by file executing in batches, combining all the data into one big csv file.\n",
    "\n",
    "Then I will utilise pyspark to calculate other features ($v_2$ to $v_{10}$). Finally there should be 139 features ready to analyze.\n",
    "\n",
    "![title](features.png)\n",
    "\n",
    "This is list of features is derived from Kercheval, A.N., Zhang, Y., 2015. Modelling high-frequency limit order book dynamics with support vector machines. Quantitative Finance 15, 1315–1329."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one corrupted file among the data. I was told to ignore it, so let's mark it away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRUPTED FILE: 'SE0000115446_SEK/8_SE0000115446_SEK_208.mat'\n",
    "corrupted = ['8_SE0000115446_SEK_208.mat']\n",
    "#sio.loadmat(corrupted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with file management. Data is stored into different folders for each stock. Find the folders and Filter out non-data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/SE0000101032_SEK',\n",
       " '../Data/SE0000115446_SEK',\n",
       " '../Data/FI0009005318_EUR',\n",
       " '../Data/FI0009007835_EUR',\n",
       " '../Data/DK0010268606_DKK']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only needed directories\n",
    "path = '../Data/'\n",
    "dirs = [path + d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "del dirs[1]\n",
    "del dirs[1]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz # to handle timezones\n",
    "\n",
    "# method to extract data from the order book\n",
    "def extract_order_book(path, deep=10):\n",
    "    # load data\n",
    "    mat = sio.loadmat(path)\n",
    "    order_book = mat['ob']\n",
    "\n",
    "    # time strip and convert to correct timezone\n",
    "    ts = pd.Series(order_book['ts'][0][0][:,0], name='ts')\n",
    "    ts = pd.to_datetime(ts, unit='ms') \n",
    "    ts = ts.dt.tz_localize('UTC').dt.tz_convert('Europe/Helsinki')\n",
    "    \n",
    "    # filter out pre-session and after-session data\n",
    "    f_ts = (ts.dt.hour > 8) & (ts.dt.hour < 18)\n",
    "    \n",
    "    # extract timestamp data to list of dataframes\n",
    "    dfs = [pd.DataFrame(ts[f_ts]).reset_index(drop=True)]\n",
    "    \n",
    "    # extract order book data\n",
    "    for c in ['ask_p', 'ask_q', 'bid_p', 'bid_q']:\n",
    "        dfs.append(pd.DataFrame(order_book[c][0, 0][f_ts, :deep], columns=[c + str(i) for i in xrange(1, deep+1)]))\n",
    "        \n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df = df.set_index('ts').tz_localize(None) # remove timezone information\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and save filenames to ensure chronologically correct data.\n",
    "f_names = []\n",
    "for d in dirs:\n",
    "    files = os.listdir(d)\n",
    "    f_names.append(natsorted(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/SE0000101032_SEK/8_SE0000101032_SEK_1.mat (1/765)\t\r"
     ]
    }
   ],
   "source": [
    "## HIDAS ÄLÄ AJA\n",
    "# filename: SE0000101032_SEK.csv\n",
    "testfile = 'SE0000101032_SEK_test.csv'\n",
    "'''\n",
    "for i in xrange(len(dirs)):\n",
    "    d = dirs[i]\n",
    "    files = f_names[i]\n",
    "    n = len(files)\n",
    "    \n",
    "    for j, f in enumerate(files):\n",
    "        # update progress\n",
    "        sys.stdout.write(\"%s (%i/%i)\\t\\r\" % (d + '/' + f, j+1, n))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        # checks for bad files\n",
    "        if not f.endswith('.mat'):\n",
    "            continue\n",
    "        if f in corrupted:\n",
    "            continue\n",
    "            \n",
    "        # extract data\n",
    "        df = extract_order_book(d+'/'+f)\n",
    "        \n",
    "        filename = f[2:18] + '.csv'\n",
    "        \n",
    "        file_exists = os.path.isfile(filename)\n",
    "        \n",
    "        with open(filename, 'a') as w:\n",
    "            \n",
    "            # if file does not exist, write headers into file\n",
    "            if file_exists:\n",
    "                df.to_csv(w, header=False)\n",
    "            else:\n",
    "                df.to_csv(w, header=True)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_1$: Order book data\n",
    "\n",
    "$v_1 = \\{ P^{ask}_{i}, Q^{ask}_{i}, P^{bid}_{i}, Q^{bid}_{i}\\}^n_{i=1}$ is read from the csv-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|ts                     |ask_p1   |ask_p2   |ask_p3   |ask_p4   |ask_p5   |ask_p6   |ask_p7   |ask_p8   |ask_p9   |\n",
      "+-----------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|2010-06-01 09:00:00.142|1128000.0|1130000.0|1140000.0|1142000.0|1143000.0|1145000.0|1150000.0|1153000.0|1160000.0|\n",
      "|2010-06-01 09:00:00.144|1128000.0|1130000.0|1140000.0|1142000.0|1143000.0|1145000.0|1150000.0|1153000.0|1160000.0|\n",
      "+-----------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "column names are: 'ts' and \n",
      "['ask_p1', 'ask_p2', 'ask_p3', 'ask_p4', 'ask_p5', 'ask_p6', 'ask_p7', 'ask_p8', 'ask_p9', 'ask_p10']\n",
      "['ask_q1', 'ask_q2', 'ask_q3', 'ask_q4', 'ask_q5', 'ask_q6', 'ask_q7', 'ask_q8', 'ask_q9', 'ask_q10']\n",
      "['bid_p1', 'bid_p2', 'bid_p3', 'bid_p4', 'bid_p5', 'bid_p6', 'bid_p7', 'bid_p8', 'bid_p9', 'bid_p10']\n",
      "['bid_q1', 'bid_q2', 'bid_q3', 'bid_q4', 'bid_q5', 'bid_q6', 'bid_q7', 'bid_q8', 'bid_q9', 'bid_q10']\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(header=\"true\", \n",
    "                                      inferSchema=\"true\",\n",
    "                                      dateFormat=\"yyyy-MM-dd HH:mm:ss\")\\\n",
    "            .load(testfile)\n",
    "    \n",
    "df.select(df.columns[:10]).show(2, False)\n",
    "\n",
    "print 'column names are: \\'ts\\' and '\n",
    "cols = df.columns\n",
    "print cols[1:11]\n",
    "print cols[11:21]\n",
    "print cols[21:31]\n",
    "print cols[31:41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_2$: Spread and mid price\n",
    "\n",
    "$v_2 = \\{ (P^{ask}_i - P^{bid}_i), \\frac{(P^{ask}_i + P^{bid}_i)}{2} \\}^n_{i=1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|spread_p1|spread_p2|spread_p3|spread_p4|spread_p5|spread_p6|spread_p7|spread_p8|spread_p9|spread_p10|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|  18000.0|  30000.0|  75000.0|  82000.0|  91000.0| 109000.0| 115000.0| 123000.0| 135000.0|  145000.0|\n",
      "|  18000.0|  30000.0|  75000.0|  82000.0|  91000.0| 109000.0| 115000.0| 123000.0| 135000.0|  145000.0|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|midprice1|midprice2|midprice3|midprice4|midprice5|midprice6|midprice7|midprice8|midprice9|midprice10|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "|1119000.0|1115000.0|1102500.0|1101000.0|1097500.0|1090500.0|1092500.0|1091500.0|1092500.0| 1092500.0|\n",
      "|1119000.0|1115000.0|1102500.0|1101000.0|1097500.0|1090500.0|1092500.0|1091500.0|1092500.0| 1092500.0|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate spreads per price level\n",
    "for i in xrange(1, 11):\n",
    "    df = df.withColumn('spread_p' + str(i), col('ask_p' + str(i)) - col('bid_p' + str(i)))\n",
    "    \n",
    "df.select(df.columns[41:]).show(2)\n",
    "\n",
    "# calculate Mid-Prices per price level\n",
    "for i in xrange(1, 11):\n",
    "    marksColumns = [col('ask_p' + str(i)), col('bid_p' + str(i))]\n",
    "    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n",
    "    \n",
    "    df = df.withColumn('midprice' + str(i), averageFunc)\n",
    "        \n",
    "df.select(df.columns[51:]).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_3$: Price differences\n",
    "\n",
    "$v_2 = \\{ \\big| P^{ask}_{i+1} - P^{ask}_i \\big|, \\big| P^{bid}_{i+1} - P^{bid}_i \\big| \\} ^{n-1}_{i=1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|ask_diff1|ask_diff2|ask_diff3|ask_diff4|ask_diff5|ask_diff6|ask_diff7|ask_diff8|ask_diff9|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|   2000.0|  10000.0|   2000.0|   1000.0|   2000.0|   5000.0|   3000.0|   7000.0|   5000.0|\n",
      "|   2000.0|  10000.0|   2000.0|   1000.0|   2000.0|   5000.0|   3000.0|   7000.0|   5000.0|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|bid_diff1|bid_diff2|bid_diff3|bid_diff4|bid_diff5|bid_diff6|bid_diff7|bid_diff8|bid_diff9|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|  10000.0|  35000.0|   5000.0|   8000.0|  16000.0|   1000.0|   5000.0|   5000.0|   5000.0|\n",
      "|  10000.0|  35000.0|   5000.0|   8000.0|  16000.0|   1000.0|   5000.0|   5000.0|   5000.0|\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculate price differences for subsequent asking and bidding prices\n",
    "# ask\n",
    "for i in xrange(1, 10):\n",
    "    df = df.withColumn('ask_diff' + str(i), F.abs(col('ask_p' + str(i+1)) - col('ask_p' + str(i))))\n",
    "\n",
    "df.select(df.columns[61:]).show(2)\n",
    "    \n",
    "# bid\n",
    "for i in xrange(1, 10):\n",
    "    df = df.withColumn('bid_diff' + str(i), F.abs(col('bid_p' + str(i+1)) - col('bid_p' + str(i))))\n",
    "\n",
    "df.select(df.columns[70:]).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_4$: Price and volume means\n",
    "\n",
    "$v_4 = \\{ \\frac{1}{n}\\sum_{i=1}^{n} P^{ask}_i,\n",
    "          \\frac{1}{n}\\sum_{i=1}^{n} P^{bid}_i, \n",
    "          \\frac{1}{n}\\sum_{i=1}^{n} Q^{ask}_i, \n",
    "          \\frac{1}{n}\\sum_{i=1}^{n} Q^{bid}_i \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+\n",
      "|meanask_p|meanbid_p|meanask_q|meanbid_q|\n",
      "+---------+---------+---------+---------+\n",
      "|1145600.0|1053300.0|   1998.2|   1813.3|\n",
      "|1145600.0|1053300.0|   2018.2|   1813.3|\n",
      "+---------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['ask_p', 'bid_p', 'ask_q', 'bid_q']:\n",
    "    marksColumns = [col(c + str(i)) for i in xrange(1, 11)]\n",
    "    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n",
    "    df = df.withColumn('mean' + c, averageFunc)\n",
    "\n",
    "df.select(df.columns[79:]).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_5$: Accumulated differences\n",
    "\n",
    "$v_4 = \\{ \\frac{1}{n}(\\sum_{i=1}^{n} P^{ask}_i - P^{bid}_i), \n",
    "          \\frac{1}{n}(\\sum_{i=1}^{n} Q^{ask}_i - Q^{bid}_i) \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+-------+-----+\n",
      "|acc_diff_pbid_q|acc_diff_p|acc_diff_q|    foo|  bar|\n",
      "+---------------+----------+----------+-------+-----+\n",
      "|        92300.0|   92300.0|     184.9|92300.0|184.9|\n",
      "|        92300.0|   92300.0|     204.9|92300.0|204.9|\n",
      "+---------------+----------+----------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = df\n",
    "\n",
    "marksColumns = [col('ask_p' + str(i)) for i in xrange(1, 11)] + [col('bid_p' + str(i)) for i in xrange(1, 11)]\n",
    "bar = (sum(x for x in marksColumns[:10]) - sum(x for x in marksColumns[10:])) / (len(marksColumns) / 2)\n",
    "foo = foo.withColumn('foo', bar)\n",
    "\n",
    "marksColumns = [col('ask_q' + str(i)) for i in xrange(1, 11)] + [col('bid_q' + str(i)) for i in xrange(1, 11)]\n",
    "bar = (sum(x for x in marksColumns[:10]) + sum(x for x in marksColumns[10:])) / (len(marksColumns) / 2)\n",
    "foo = foo.withColumn('bar', bar)\n",
    "\n",
    "foo.select(foo.columns[83:]).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+\n",
      "|acc_diff_pbid_q|acc_diff_p|acc_diff_q|\n",
      "+---------------+----------+----------+\n",
      "|        92300.0|   92300.0|     184.9|\n",
      "|        92300.0|   92300.0|     204.9|\n",
      "+---------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marksColumns = [col('ask_p' + str(i)) for i in xrange(1, 11)] + [col('bid_p' + str(i)) for i in xrange(1, 11)]\n",
    "meanOfDiffs = (sum(x for x in marksColumns[:10]) - sum(x for x in marksColumns[10:])) / (len(marksColumns) / 2)\n",
    "df = df.withColumn('acc_diff_p', meanOfDiffs)\n",
    "\n",
    "marksColumns = [col('ask_q' + str(i)) for i in xrange(1, 11)] + [col('bid_q' + str(i)) for i in xrange(1, 11)]\n",
    "meanOfDiffs = (sum(x for x in marksColumns[:10]) - sum(x for x in marksColumns[10:])) / (len(marksColumns) / 2)\n",
    "df = df.withColumn('acc_diff_q', meanOfDiffs)\n",
    "\n",
    "\n",
    "df.select(df.columns[83:]).show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
